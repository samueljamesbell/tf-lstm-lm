data:
    batch_size: 20
    num_steps: 35

lstm:
    hidden_dims: 1500
    num_layers: 2
    init_scale: 0.04
    max_gradient_norm: 10
    dropout:
        keep_prob: 0.35

training:
    num_epochs: 55
    terminate_after: 8
    learning_rate_decay: 0.87
    decay_after: 14

vocab:
    max_size: 10000
    rare_word_threshold: 1

