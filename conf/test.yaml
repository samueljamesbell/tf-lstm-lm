data:
    batch_size: 3
    num_steps: 8

lstm:
    hidden_dims: 300
    num_layers: 2
    init_scale: 0.05
    dropout:
        keep_prob: 0.6

training:
    num_epochs: 1
    terminate_after: 4
    learning_rate_decay: 0.8
    decay_after: 2

vocab:
    max_size: 10000
    rare_word_threshold: 1

